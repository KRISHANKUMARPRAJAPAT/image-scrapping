{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a18054c-7480-45e7-aba5-8c0ea9ca7a20",
   "metadata": {},
   "source": [
    "# Go to this given URL and solve the following questions\n",
    "\n",
    "- URL: https://www.youtube.com/@PW-Foundation/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10f26c-45ad-449d-8bba-373a1346c918",
   "metadata": {},
   "source": [
    "# Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6127b6fe-2e1a-4a65-8152-37ab45681997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans; 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2570ce7c-9c96-4766-b820-e854ddf9b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.11.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0c860e-7c24-417d-9356-f4ec2516fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the YouTube channel URL\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "# Send a GET request to the channel URL\n",
    "response = requests.get(channel_url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all the video links on the page\n",
    "video_links = soup.select(\"a.yt-simple-endpoint.style-scope.yt-formatted-string\")\n",
    "\n",
    "# Extract the video URLs of the first five videos\n",
    "video_urls = []\n",
    "for link in video_links[:5]:\n",
    "    video_url = \"https://www.youtube.com\" + link[\"href\"]\n",
    "    video_urls.append(video_url)\n",
    "\n",
    "# Print the video URLs\n",
    "for url in video_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ad9ac-85f9-4f52-a746-7d536f651818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f0cd151-c569-4584-b8d2-77b3e5d90d99",
   "metadata": {},
   "source": [
    "# Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef169cea-a69a-4303-9496-1b355f744ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans:  2\n",
    "\n",
    "# Define the YouTube channel URL\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "# Send a GET request to the channel URL\n",
    "response = requests.get(channel_url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all the thumbnail images on the page\n",
    "thumbnail_images = soup.select(\"a.yt-simple-endpoint.style-scope.ytd-thumbnail > img\")\n",
    "\n",
    "# Extract the URLs of the video thumbnails for the first five videos\n",
    "thumbnail_urls = []\n",
    "for image in thumbnail_images[:5]:\n",
    "    thumbnail_url = image[\"src\"]\n",
    "    thumbnail_urls.append(thumbnail_url)\n",
    "\n",
    "# Print the URLs of the video thumbnails\n",
    "for url in thumbnail_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91a20a-b87d-483c-bb07-93c01941853c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4203e151-1448-4c10-b491-7db370d9b83d",
   "metadata": {},
   "source": [
    "# Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce199ac-db9b-401f-a132-b429b8e237a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans:  3\n",
    "\n",
    "# Define the YouTube channel URL\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "# Send a GET request to the channel URL\n",
    "response = requests.get(channel_url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all the video titles on the page\n",
    "video_titles = soup.select(\"a#video-title\")\n",
    "\n",
    "# Extract the titles of the first five videos\n",
    "titles = []\n",
    "for title in video_titles[:5]:\n",
    "    video_title = title.text.strip()\n",
    "    titles.append(video_title)\n",
    "\n",
    "# Print the titles of the first five videos\n",
    "for title in titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07f237-0f1b-4807-9660-4974570d6134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "785999d0-1dca-451e-849b-d42d29bd760d",
   "metadata": {},
   "source": [
    "# Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b978e30-def8-4fe9-a070-e8fad2ea2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans: 4\n",
    "\n",
    "# Define the YouTube channel URL\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "# Send a GET request to the channel URL\n",
    "response = requests.get(channel_url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all the view count elements on the page\n",
    "view_counts = soup.select(\"span.style-scope.ytd-grid-video-renderer > span:nth-child(1)\")\n",
    "\n",
    "# Extract the view counts of the first five videos\n",
    "views = []\n",
    "for count in view_counts[:5]:\n",
    "    view_count = count.text.strip()\n",
    "    views.append(view_count)\n",
    "\n",
    "# Print the view counts of the first five videos\n",
    "for view in views:\n",
    "    print(view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1792de-fb41-4cb8-ad01-f98187df6ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c20637-f787-4209-9d6b-d7a652d5c664",
   "metadata": {},
   "source": [
    "# Q5. Write a python program to extract the time of posting of video for the first five videos.\n",
    "- Note: Save all the data scraped in the above questions in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dab5129-0489-4fe2-8e17-b7cb0d8f8474",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_titles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     27\u001b[0m     view_count \u001b[38;5;241m=\u001b[39m view_counts[i]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m     time_posted \u001b[38;5;241m=\u001b[39m time_elements[i]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the YouTube channel URL\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "# Send a GET request to the channel URL\n",
    "response = requests.get(channel_url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all the video titles on the page\n",
    "video_titles = soup.select(\"a#video-title\")\n",
    "\n",
    "# Find all the view count elements on the page\n",
    "view_counts = soup.select(\"span.style-scope.ytd-grid-video-renderer > span:nth-child(1)\")\n",
    "\n",
    "# Find all the time elements on the page\n",
    "time_elements = soup.select(\"a#video-title > span.style-scope.ytd-thumbnail-overlay-time-status-renderer\")\n",
    "\n",
    "# Extract the titles, view counts, and time of posting for the first five videos\n",
    "data = []\n",
    "for i in range(5):\n",
    "    title = video_titles[i].text.strip()\n",
    "    view_count = view_counts[i].text.strip()\n",
    "    time_posted = time_elements[i].text.strip()\n",
    "    data.append([title, view_count, time_posted])\n",
    "\n",
    "# Save the scraped data in a CSV file\n",
    "filename = \"youtube_data.csv\"\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"View Count\", \"Time Posted\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data scraped from YouTube channel saved in '{filename}' file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc218a-00c2-40f0-958c-a1823cae6d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
